# 针对嵌入式设备的量化部署流程





## 神经网络框架与训练

使用专用的神经网络框架可以大大加快神经网络的研发和训练，从而提升科研/生产效率。目前最主流的框架就是TensorFlow和PyTorch

### TensorFlow与Keras

TensorFlow是由谷歌大脑团队的研究人员和工程师开发的，它是深度学习领域中最常用的软件库

它支持多种语言，常用的有Python和C，同时支持GPU加速

TensorFlow中的所有计算都会被转化为计算图上的节点。整个框架通过**数据流图**的形式来表述计算的编程系统，每个计算都是图上的一个**节点**，而节点之间的**边**描述了计算之间的依赖关系

Keras是用Python编写的框架，可以基于TensorFlow运行

TensorFlow的接口不太适合新手，而Keras提供了高层的API，可以实现快速开发。Keras支持CNN和RNN，可以在CPU和GPU上无缝运行。需要注意：**Keras中模型的层是按顺序定义的**

### PyTorch

PyTorch是Torch的API接口，可用于建立深度神经网络和执行张量计算。Torch是一个基于Lua的框架，而PyTorch则运行在Python上

PyTorch基于张量计算（多维数组），就像numpy的ndarray一样，它也可以在GPU上运行

不同于Tensorflow的数据流图，PyTorch使用动态计算图，它的的Autograd软件包从张量生成计算图，并自动计算梯度。因此它可以实现在运行时构建计算图形，甚至在运行时也可以对这些图形进行更改。当不知道创建神经网络需要多少内存的情况下，这个功能便很有价值

## 量化工具与使用

**量化**（Quantization）即通过减少表示每个权重所需的比特数来压缩原始网络。

神经网络模型的权重和偏置（参数）值都是32位浮点数，这保证神经网络模型的精度，但是浮点数的运算难度相比整形大得多，通过量化技术可以把32浮点数变为整数，从而减小计算量和参数体积。目前性能最稳定的就是INT8的模型量化技术，它将浮点数变为8位整数，相对于原始模型的FP32计算，INT8量化可将模型大小减少4倍，将内存带宽要求减少4倍，设备也不需要FPU，从而让只支持计算INT8的硬件（没有FPU的设备）加速2到4倍。

然而量化主要是一种加速前向推理的技术，这导致目前**绝大部分的量化算子仅支持前向传递**

基础的量化方法有三种：

* 训练后动态量化（**Post Training Dynamic Quantization**）：在浮点模型训练完成后进行量化，其中权重会被提前量化，偏置不会被量化，activation则会在前向推理的过程中被动态量化。全部过程中需要根据实际运算的浮点数据范围每层计算一次scale和zero_point，再进行量化。

	由于该方法计算量较大，需要多次计算量化参数，所以不常用

* 训练后静态量化（**Post Training Static Quantization**）：在浮点模型训练完成后进行量化，权重会被提前量化，而activation会基于之前校准过程中记录下固定的scale和zero_point进行量化，从而避免了反复计算参数

* 量化感知训练（**Quantization Aware Training**）：一些模型在训练和量化的过程中会发生较严重的精度损失，因此需要使用量化感知训练。这个过程就是在训练中*模拟*量化过程——数据仍是FP32格式，但是实际值的间隔会受到量化参数的限制

### PyTorch自带量化

**一般来说偏置（bias）不进行量化，仍保持FP32（32位浮点数）的数据类型；而权重（weight）会根据原始数据进行量化；activation会因为每次输入数据的不同，导致数据范围每次都是不同的，所以在量化过程中专门会有一个校准过程，即提前准备一个小的校准数据集，在测试这个校准数据集的时候会记录每一次的activation的数据范围，然后根据记录值确定一个固定的范围**



### TensorRT

TensorRT是英伟达针对自家平台（N卡）做的一套加速包（或者说SDK），包含一个深度学习推理优化器和运行时库，可以通过降低算法延迟、增加存储吞吐量等方法加速深度学习推理过程，不过在优化过程中可能会导致算法精度下降的问题。

它建立在老黄的CUDA上，使用了里面的底层库、开发工具，所以对于N卡原生适配。

TensorRT提供int8和fp16等优化方式，还支持了浮点数转定点数等优化方法

目前TensorRT支持PyTorch、TensorFlow、Matlab等主流NN框架，还支持从onnx直接解析神经网络模型。

TensorRT需要使用专用的模型格式，支持PyTorch的pt、TensorFlow的tf、通用的onnx等模型输入量化

**TensorRT安装过程可以参考官网或其他教程，这里不再赘述**。一般只要Cuda版本正确，安装对应的TensorRT就可以了（尽量避免在Windows上安装，会遇到很难受的兼容性问题）





### NCNN









