# 针对嵌入式设备的量化部署流程





## 神经网络框架与训练

使用专用的神经网络框架可以大大加快神经网络的研发和训练，从而提升科研/生产效率。目前最主流的框架就是TensorFlow和PyTorch

### TensorFlow与Keras

TensorFlow是由谷歌大脑团队的研究人员和工程师开发的，它是深度学习领域中最常用的软件库

它支持多种语言，常用的有Python和C，同时支持GPU加速

TensorFlow中的所有计算都会被转化为计算图上的节点。整个框架通过**数据流图**的形式来表述计算的编程系统，每个计算都是图上的一个**节点**，而节点之间的**边**描述了计算之间的依赖关系

Keras是用Python编写的框架，可以基于TensorFlow运行

TensorFlow的接口不太适合新手，而Keras提供了高层的API，可以实现快速开发。Keras支持CNN和RNN，可以在CPU和GPU上无缝运行。需要注意：**Keras中模型的层是按顺序定义的**

### PyTorch

PyTorch是Torch的API接口，可用于建立深度神经网络和执行张量计算。Torch是一个基于Lua的框架，而PyTorch则运行在Python上

PyTorch基于张量计算（多维数组），就像numpy的ndarray一样，它也可以在GPU上运行

不同于Tensorflow的数据流图，PyTorch使用动态计算图，它的的Autograd软件包从张量生成计算图，并自动计算梯度。因此它可以实现在运行时构建计算图形，甚至在运行时也可以对这些图形进行更改。当不知道创建神经网络需要多少内存的情况下，这个功能便很有价值

## 量化工具与使用







### TensorRT

TensorRT是英伟达针对自家平台（N卡）做的一套加速包（或者说SDK），包含一个深度学习推理优化器和运行时库，可以通过降低算法延迟、增加存储吞吐量等方法加速深度学习推理过程，不过在优化过程中可能会导致算法精度下降的问题。

它建立在老黄的CUDA上，使用了里面的底层库、开发工具，所以对于N卡原生适配。

TensorRT提供int8和fp16等优化方式，还支持了浮点数转定点数等优化方法

目前TensorRT支持PyTorch、TensorFlow、Matlab等主流NN框架，还支持从onnx直接解析神经网络模型。

TensorRT需要使用专用的模型格式，支持PyTorch的pt、TensorFlow的tf、通用的onnx等模型输入量化

**TensorRT安装过程可以参考官网或其他教程，这里不再赘述**。一般只要Cuda版本正确，安装对应的TensorRT就可以了（尽量避免在Windows上安装，会遇到很难受的兼容性问题）

使用tensorrtx



### NCNN









